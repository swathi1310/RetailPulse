# -*- coding: utf-8 -*-
"""Retail_Data_Analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HpN-iG2C0SagokMMLJuCJJlKSkq3F8dr
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
# %matplotlib inline
import matplotlib.pyplot as plt
import matplotlib.lines as mlines
from mpl_toolkits.mplot3d import Axes3D
import seaborn as sns
from sklearn.model_selection import train_test_split, learning_curve
from sklearn.metrics import average_precision_score
from xgboost.sklearn import XGBClassifier
from xgboost import plot_importance, to_graphviz
from sklearn.utils import resample
from sklearn.linear_model import LogisticRegression
import warnings
warnings.filterwarnings("ignore", category=DeprecationWarning)
from google.colab import drive
drive.mount('/content/drive')

dataframe=pd.read_csv('/content/drive/MyDrive/Updated_Large_ProductIDs_Within_Categories.csv')

print(dataframe.info())

dataframe['PurchaseDate'] = pd.to_datetime(dataframe['PurchaseDate'],format='%d-%m-%Y', errors='coerce')

# print(invalid_rows)
print(dataframe.info())

# Now you can fill missing values (NaT) with the mode or any other approach
dataframe['PurchaseDate'].fillna(dataframe["PurchaseDate"].mode()[0], inplace=True)

dataframe.info()

import requests
import json
def get_holidays(years,API_KEY, country_code='US'):
  base_url = "https://calendarific.com/api/v2/holidays"
  holidays_in_range = []
  for year in years:
    # Send a GET request to the API with parameters
    params = {
        'api_key': API_KEY,
        'country': country_code,
        'year': year
    }
    response = requests.get(base_url, params=params)
    data = response.json()
    if 'response' in data and 'holidays' in data['response']:
            for holiday in data['response']['holidays']:
                holiday_date = holiday['date']['iso']
                holiday_name = holiday['name']
                holidays_in_range.append({'date': holiday_date, 'name': holiday_name})
  return holidays_in_range

print(dataframe.info())
api_key = "bfaohNRQkv0R9vo3QSQaU1TnbaZwVmWk"
country_code = 'US'
dataframe['PurchaseDate'] = pd.to_datetime(dataframe['PurchaseDate'])


# Get the start (earliest) and end (latest) dates from the 'PurchaseDate' column
start_date = dataframe['PurchaseDate'].min()
end_date = dataframe['PurchaseDate'].max()
years = range(start_date.year, end_date.year + 1)
holidays_in_range = get_holidays(years, api_key, country_code)
# Create a dictionary of holiday dates with their names for fast lookup
holiday_dict = {holiday['date']: holiday['name'] for holiday in holidays_in_range}

# Add 'isHoliday' column (True if date is a holiday, False otherwise)
dataframe['isHoliday'] = dataframe['PurchaseDate'].dt.strftime('%Y-%m-%d').isin(holiday_dict.keys())

# Add 'HolidayName' column (holiday name if date is a holiday, otherwise None)
dataframe['HolidayName'] = dataframe['PurchaseDate'].dt.strftime('%Y-%m-%d').map(holiday_dict).fillna("None")
dataframe.head()

dataframe.info()

# Check for missing values in the dataset
print("Missing Values Summary:")
print(dataframe.isnull().sum())

# Check for unique values in categorical columns (to identify inconsistencies)
categorical_columns = ['Gender', 'Location', 'Size', 'Category']
for col in categorical_columns:
    print(f"\nUnique values in {col}:")
    print(dataframe[col].unique())

dataframe['Gender'].replace('Unknown',dataframe['Gender'].mode()[0],inplace=True)
dataframe['Location'].replace('InvalidCity',dataframe['Location'].mode()[0],inplace=True)

for col in categorical_columns:
    print(f"\nUnique values in {col}:")
    print(dataframe[col].unique())

print(dataframe.isnull().sum())

print(dataframe.info())

dataframe['Age'].fillna(dataframe['Age'].mean(), inplace=True)
dataframe['Total_Value'].fillna(dataframe['Total_Value'].mean(), inplace=True)

print(dataframe.isnull().sum())

print(dataframe.info())

# Check for rows where Price is less than 0
invalid_price_rows = dataframe[dataframe['Quantity'] < 0]

# Count the number of invalid rows
count_invalid_prices = invalid_price_rows.shape[0]

# Display the rows with invalid prices
print(invalid_price_rows)

# Output the count of invalid rows
print(f"Number of rows with Quantity less than 0: {count_invalid_prices}")

dataframe['Quantity'] = dataframe['Quantity'].replace(-1, dataframe['Quantity'].median())
invalid_quantity_count = (dataframe['Quantity'] < 0).sum()
print(f"Number of rows with invalid Quantity after replacement: {invalid_quantity_count}")

# Check for duplicate rows
duplicate_count = dataframe.duplicated().sum()
print(f"\nNumber of duplicate rows: {duplicate_count}")

dataframe.info()

print(dataframe['Age'].unique)
dataframe['Age'] = dataframe['Age'].apply(lambda x: round(x) if pd.notnull(x) else x)  # Round ages to nearest integer

# Step 9: Perform exploratory analysis to identify trends and outliers
numerical_columns = ['Quantity', 'Price', 'Age', 'Total_Value']
plt.figure(figsize=(15, 10))
for i, col in enumerate(numerical_columns, 1):
    plt.subplot(2, 2, i)
    sns.boxplot(dataframe[col])
    plt.title(f"Boxplot of {col}")
plt.tight_layout()
plt.show()

# Step 10: Handle outliers in Total_Value
# Calculate the Interquartile Range (IQR)
Q1 = dataframe['Total_Value'].quantile(0.25)
Q3 = dataframe['Total_Value'].quantile(0.75)
IQR = Q3 - Q1

stricter_multiplier = 1.2  # Adjust this value as needed
lower_bound = Q1 - stricter_multiplier * IQR
upper_bound = Q3 + stricter_multiplier * IQR

# Filter out rows with outliers in Total_Value
dataframe = dataframe[(dataframe['Total_Value'] >= lower_bound) & (dataframe['Total_Value'] <= upper_bound)]

# Step 9: Perform exploratory analysis to identify trends and outliers
numerical_columns = ['Quantity', 'Price', 'Age', 'Total_Value']
plt.figure(figsize=(15, 10))
for i, col in enumerate(numerical_columns, 1):
    plt.subplot(2, 2, i)
    sns.boxplot(dataframe[col])
    plt.title(f"Boxplot of {col}")
plt.tight_layout()
plt.show()

customer_group = dataframe.groupby('Customer_ID')
dataframe['Avg_Spend_Per_Customer'] = dataframe['Customer_ID'].map(customer_group['Total_Value'].mean())
dataframe['Repeat_Purchases'] = dataframe['Customer_ID'].map(customer_group['Customer_ID'].transform('count'))

# Store-level metrics: total sales and product category performance
store_group = dataframe.groupby('Store_ID')
dataframe['Total_Sales_Per_Store'] = dataframe['Store_ID'].map(store_group['Total_Value'].sum())
category_group = dataframe.groupby('Category')
dataframe['Avg_Product_Sales_Per_Category'] = dataframe['Category'].map(category_group['Total_Value'].mean())

import pandas as pd
import matplotlib.pyplot as plt

# Load the dataset

# Calculate the top 10 performing products
product_performance = dataframe.groupby(['Product_ID', 'Category'])['Total_Value'].sum().reset_index()
product_performance = product_performance.sort_values(by='Total_Value', ascending=False).head(10)
print("Top 10 Performing Products:")
print(product_performance)

# Plotting the graph
plt.figure(figsize=(12, 8))
plt.barh(
    product_performance['Product_ID'] + " (" + product_performance['Category'] + ")",
    product_performance['Total_Value'],
    color='skyblue'
)
plt.xlabel('Total Sales Value')
plt.ylabel('Product (Category)')
plt.title('Top 10 Performing Products with Categories')
plt.gca().invert_yaxis()  # Invert y-axis for better readability
plt.tight_layout()
plt.show()

import pandas as pd
import requests
import time

# Function to fetch state from city using Nominatim API
def get_state_from_city_nominatim(city_name):
    try:
        url = f"https://nominatim.openstreetmap.org/search?city={city_name}&format=json"
        response = requests.get(url, headers={"User-Agent": "Mozilla/5.0"})
        if response.status_code == 200 and response.json():
            data = response.json()
            # Extract 'display_name' and split by commas
            display_name = data[0].get("display_name", "")
            parts = display_name.split(", ")
            # Assume state is second-to-last (before country)
            if len(parts) > 2:
                state = parts[-2]
                return state
    except Exception as e:
        print(f"Error fetching state for {city_name}: {e}")
    return None



# Step 1: Get unique cities from the dataset
unique_cities = dataframe['Location'].unique()

# Step 2: Query the API for each unique city and store results in a dictionary
city_to_state = {}
for city in unique_cities:
    print(f"Fetching state for city: {city}")
    city_to_state[city] = get_state_from_city_nominatim(city)
  # Add a delay of 1 second to avoid rate-limiting

# Step 3: Map the state information back to the original DataFrame
dataframe['State'] = dataframe['Location'].map(city_to_state)

# Step 4: Print the updated DataFrame
print(dataframe.head(5))

import pandas as pd
import matplotlib.pyplot as plt

# Ensure dataframe has the 'State' and 'Total_Value' columns
assert 'State' in dataframe.columns, "State column is missing in the dataset"
assert 'Total_Value' in dataframe.columns, "Total_Value column is missing in the dataset"

# Step 1: Calculate total sales per state
total_sales_by_state = dataframe.groupby('State')['Total_Value'].sum().reset_index()
total_sales_by_state = total_sales_by_state.sort_values(by='Total_Value', ascending=False).head(10)

# Step 2: Bar Chart for Top 10 State-Level Sales
plt.figure(figsize=(12, 8))
plt.barh(total_sales_by_state['State'], total_sales_by_state['Total_Value'], color='skyblue')
plt.xlabel('Total Sales Value')
plt.ylabel('State')
plt.title('Top 10 State-Level Sales Performance')
plt.gca().invert_yaxis()
plt.tight_layout()
plt.show()

# Group by Store_ID and calculate total sales value
store_performance = dataframe.groupby('Store_ID')['Total_Value'].sum().reset_index()

# Sort and get the top 10 stores
store_performance = store_performance.sort_values(by='Total_Value', ascending=False).head(10)

# Extract Store_ID and Location columns
store_details = dataframe[['Store_ID', 'Location']]

# Merge with store details
store_performance = store_performance.merge(store_details, on='Store_ID', how='left')

# Ensure only top 10 unique performing stores with their locations are displayed
store_performance = store_performance.drop_duplicates(subset=['Store_ID'])

# Display the results
print("\nTop 10 Performing Stores with Locations:")
print(store_performance)

# Plotting the graph
plt.figure(figsize=(12, 8))
plt.barh(
    store_performance['Store_ID'].astype(str) + " (" + store_performance['Location'] + ")",
    store_performance['Total_Value'],
    color='skyblue'
)
plt.xlabel('Total Sales Value')
plt.ylabel('Store (Location)')
plt.title('Top 10 Performing Stores with Locations')
plt.gca().invert_yaxis()  # Invert y-axis for better readability
plt.tight_layout()
plt.show()

# Check for missing or invalid values in 'State' and 'isHoliday'
print(dataframe[['State', 'isHoliday']].isnull().sum())
print(dataframe['isHoliday'].unique())
dataframe['isHoliday'].fillna(False, inplace=True)  # Assume non-holiday for missing data
dataframe['State'].fillna('Unknown', inplace=True)  # Replace missing state with a placeholder

# Group by 'isHoliday' and 'State' to calculate average and total sales
holiday_sales = dataframe.groupby(['isHoliday', 'State'])['Total_Value'].agg(['mean', 'sum']).reset_index()

# Rename columns for clarity
holiday_sales.rename(columns={'mean': 'Avg_Sales', 'sum': 'Total_Sales'}, inplace=True)

# Preview the data
print(holiday_sales.head(30))

# Pivot table for holiday vs non-holiday sales comparison
holiday_pivot = holiday_sales.pivot(index='State', columns='isHoliday', values='Avg_Sales')
holiday_pivot.columns = ['Non-Holiday Sales', 'Holiday Sales']

# Add a column for the sales increase/decrease percentage
holiday_pivot['% Change'] = ((holiday_pivot['Holiday Sales'] - holiday_pivot['Non-Holiday Sales']) /
                             holiday_pivot['Non-Holiday Sales']) * 100

# Preview the pivot table
print(holiday_pivot.head())

# Sort states by % change
top_holiday_states = holiday_pivot.sort_values(by='% Change', ascending=False).head(10)

# Display top states
print("Top States with Highest Holiday Sales Impact:")
print(top_holiday_states)

import matplotlib.pyplot as plt

# Plot the percentage change for top states
plt.figure(figsize=(12, 8))
plt.barh(top_holiday_states.index, top_holiday_states['% Change'], color='skyblue')
plt.xlabel('Percentage Change in Sales (%)')
plt.ylabel('State')
plt.title('Top States with Highest Holiday Sales Impact')
plt.gca().invert_yaxis()
plt.tight_layout()
plt.show()

# Filter for top states
top_states_data = holiday_sales[holiday_sales['State'].isin(top_holiday_states.index)]

# Plot side-by-side bars
import seaborn as sns

plt.figure(figsize=(12, 8))
sns.barplot(data=top_states_data, x='State', y='Avg_Sales', hue='isHoliday')
plt.title('Holiday vs Non-Holiday Sales by Top States')
plt.xlabel('State')
plt.ylabel('Average Sales')
plt.tight_layout()
plt.show()

# Calculate thresholds
spend_threshold = dataframe['Avg_Spend_Per_Customer'].quantile(0.75)
repeat_threshold = dataframe['Repeat_Purchases'].median()

print(f"High Spender Threshold: {spend_threshold}")
print(f"Frequent Shopper Threshold: {repeat_threshold}")

# Add segmentation labels
dataframe['High_Spender'] = dataframe['Avg_Spend_Per_Customer'] > spend_threshold
dataframe['Frequent_Shopper'] = dataframe['Repeat_Purchases'] > repeat_threshold

# Combine labels to identify customer types
def segment_customer(row):
    if row['High_Spender'] and row['Frequent_Shopper']:
        return 'High Spender & Frequent Shopper'
    elif row['High_Spender']:
        return 'High Spender'
    elif row['Frequent_Shopper']:
        return 'Frequent Shopper'
    else:
        return 'Occasional Shopper'

dataframe['Customer_Segment'] = dataframe.apply(segment_customer, axis=1)

# Group by State and Segment
state_segments = dataframe.groupby(['State', 'Customer_Segment']).size().reset_index(name='Count')

# Pivot to make the data easier to analyze
state_segments_pivot = state_segments.pivot(index='State', columns='Customer_Segment', values='Count').fillna(0)

# Add percentages
state_segments_pivot['Total_Customers'] = state_segments_pivot.sum(axis=1)
for segment in ['High Spender & Frequent Shopper', 'High Spender', 'Frequent Shopper', 'Occasional Shopper']:
    if segment in state_segments_pivot.columns:
        state_segments_pivot[f'{segment} %'] = (state_segments_pivot[segment] / state_segments_pivot['Total_Customers']) * 100

# Preview the pivoted data
print(state_segments_pivot.head())

# Aggregate sales by date
sales_trends = dataframe.groupby('PurchaseDate')['Total_Value'].sum().reset_index()


# Extract the month and year
dataframe['YearMonth'] = dataframe['PurchaseDate'].dt.to_period('M')

# Group by YearMonth and sum Total_Value to find highs and lows
monthly_sales = dataframe.groupby('YearMonth')['Total_Value'].sum().reset_index()
monthly_sales['YearMonth'] = monthly_sales['YearMonth'].astype(str)

# Plot the monthly sales trends
plt.figure(figsize=(12, 6))
plt.plot(monthly_sales['YearMonth'], monthly_sales['Total_Value'], marker='o')
plt.title('Monthly Sales Trends')
plt.xlabel('Month-Year')
plt.ylabel('Total Sales Value')
plt.xticks(rotation=45)
plt.grid(True)
plt.tight_layout()
plt.show()

# Compare holiday and non-holiday sales
holiday_sales = dataframe.groupby('isHoliday')['Total_Value'].mean().reset_index()
sns.barplot(data=holiday_sales, x='isHoliday', y='Total_Value')
plt.title('Average Sales: Holidays vs. Non-Holidays')
plt.xlabel('Holiday')
plt.ylabel('Average Sales')
plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Analyze customer segments
segments = dataframe.groupby(['Customer_Segment', 'Gender'])['Customer_ID'].count().reset_index()
segments.rename(columns={'Customer_ID': 'Count'}, inplace=True)

# Plot customer segmentation by gender
plt.figure(figsize=(10, 6))  # Adjusted figure size for better clarity
sns.barplot(
    data=segments,
    x='Customer_Segment',
    y='Count',
    hue='Gender',
    palette='Set2'  # Set a custom color palette
)
plt.title('Customer Segmentation by Gender', fontsize=16)
plt.xlabel('Customer Segment', fontsize=12)
plt.ylabel('Number of Customers', fontsize=12)
plt.xticks(rotation=45)  # Rotate x-axis labels for better readability
plt.tight_layout()
plt.show()

# Pivot table of spending by age group and location
# Limit the number of age groups by creating bins
dataframe['Age_Group'] = pd.cut(
    dataframe['Age'],
    bins=[0,18, 25, 35, 45, 55, 65, 80],
    labels=['0-18','18-25', '26-35', '36-45', '46-55', '56-65', '66-80']

)

# Recalculate spending by age group and location
spending_by_age_location = dataframe.pivot_table(
    index='Location',
    columns='Age_Group',
    values='Avg_Spend_Per_Customer',
    aggfunc='mean'
)

# Filter for the top 10 locations based on average spend
top_locations = spending_by_age_location.mean(axis=1).nlargest(10).index
filtered_spending = spending_by_age_location.loc[top_locations]

# Plot updated heatmap
plt.figure(figsize=(10, 6))  # Adjust figure size for better readability
sns.heatmap(
    filtered_spending,
    cmap='coolwarm',
    annot=True,  # Include annotations for clarity
    fmt='.1f',  # Format annotations to one decimal place
    linewidths=0.5,  # Add space between cells
    cbar_kws={'label': 'Average Spend ($)'}  # Add colorbar label
)
plt.title('Average Spend by Age Group and Top Locations', fontsize=16)
plt.xlabel('Age Group', fontsize=12)
plt.ylabel('Location', fontsize=12)
plt.tight_layout()
plt.show()

# Filter for top 10 locations based on average spend for better clarity

from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
import pandas as pd



# Prepare data for clustering: Use Age, Total_Value, and Quantity as features for segmentation
clustering_data = dataframe[['Age', 'Total_Value', 'Quantity']].dropna()

# Scale the data for clustering
scaler = StandardScaler()
scaled_data = scaler.fit_transform(clustering_data)

# Apply K-Means clustering
kmeans = KMeans(n_clusters=4, random_state=42)
clustering_data['Cluster'] = kmeans.fit_predict(scaled_data)

# Add cluster labels back to the main dataset
dataframe['Cluster'] = clustering_data['Cluster']

# Loyalty Segmentation: Calculate average spend per customer and lifetime value
customer_loyalty = dataframe.groupby('Customer_ID').agg({
    'Total_Value': ['sum', 'mean'],  # Lifetime Value and Average Spend
    'Quantity': 'sum'  # Total Quantity Purchased
}).reset_index()

customer_loyalty.columns = ['Customer_ID', 'Lifetime_Value', 'Average_Spend', 'Total_Quantity']

# Sort customers by Lifetime Value to identify high-value customers
high_value_customers = customer_loyalty.sort_values(by='Lifetime_Value', ascending=False)

# Display the high-value customer data
print(high_value_customers.head())

# Save the results if needed
high_value_customers.to_csv('High_Value_Customers.csv', index=False)

# Top Spending Locations: Compare total spending across different locations
top_spending_locations = dataframe.groupby('Location').agg({
    'Total_Value': 'sum'
}).sort_values(by='Total_Value', ascending=False).reset_index()

# Demographic Patterns: Study how age groups or genders vary across locations
demographic_patterns = dataframe.groupby(['Location', 'Gender']).agg({
    'Age': 'mean',
    'Total_Value': 'sum'
}).reset_index()

# Underperforming Locations: Identify locations with lower-than-average spending
# Calculate the global average spending
average_spending = dataframe['Total_Value'].mean()

# Check if there are any underperforming locations
location_spending = dataframe.groupby('Location').agg({
    'Total_Value': 'sum'
}).reset_index()

# Define a threshold (e.g., below 80% of the average spending)
# Calculate the total spending for each location
total_spending_by_location = dataframe.groupby('Location')['Total_Value'].sum()
location_spending = dataframe.groupby('Location').agg({
    'Total_Value': 'sum'
}).reset_index()

# Define a new criterion: Bottom 20% of locations by spending
threshold = location_spending['Total_Value'].quantile(0.2)
underperforming_locations = location_spending[location_spending['Total_Value'] <= threshold]

# Display results
print("\nTop Spending Locations:")
print(location_spending.sort_values(by='Total_Value', ascending=False).head())

print("\nUnderperforming Locations (Bottom 20% by Spending):")
print(underperforming_locations)
print("\nDemographic Patterns Across Locations:")
print(demographic_patterns.head())

# Optionally save results
location_spending.to_csv('Location_Spending.csv', index=False)
underperforming_locations.to_csv('Underperforming_Locations.csv', index=False)

# Optionally, save the results
top_spending_locations.to_csv('Top_Spending_Locations.csv', index=False)
demographic_patterns.to_csv('Demographic_Patterns.csv', index=False)
underperforming_locations.to_csv('Underperforming_Locations.csv', index=False)

# Convert PurchaseDate to datetime format for time-based analysis
dataframe['PurchaseDate'] = pd.to_datetime(dataframe['PurchaseDate'], format='%d-%m-%Y')

# Extract year, month, and day of the week for seasonal analysis
dataframe['Year'] = dataframe['PurchaseDate'].dt.year
dataframe['Month'] = dataframe['PurchaseDate'].dt.month
dataframe['DayOfWeek'] = dataframe['PurchaseDate'].dt.dayofweek

# Seasonal Analysis: Monthly spending trends
monthly_trends = dataframe.groupby(['Year', 'Month']).agg({
    'Total_Value': 'sum',
    'Customer_ID': 'count'  # Count of purchases as a proxy for customer count
}).reset_index()

# Growth Trends: Yearly total spending and customer counts
yearly_trends = dataframe.groupby('Year').agg({
    'Total_Value': 'sum',
    'Customer_ID': 'count'
}).reset_index()

# Rename columns for clarity
monthly_trends.rename(columns={'Customer_ID': 'Purchase_Count'}, inplace=True)
yearly_trends.rename(columns={'Customer_ID': 'Purchase_Count'}, inplace=True)

# Display results
print("\nMonthly Spending Trends:")
print(monthly_trends.head())

print("\nYearly Growth Trends:")
print(yearly_trends.head())

# Optionally save results
monthly_trends.to_csv('Monthly_Spending_Trends.csv', index=False)
yearly_trends.to_csv('Yearly_Growth_Trends.csv', index=False)

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier, GradientBoostingRegressor
from sklearn.linear_model import LinearRegression
from sklearn.metrics import classification_report, mean_squared_error
from sklearn.preprocessing import LabelEncoder, MinMaxScaler



# Preprocess data for Customer Churn Prediction
# Assume churn is defined as customers with no purchases in the last year
dataframe['PurchaseDate'] = pd.to_datetime(dataframe['PurchaseDate'], format='%d-%m-%Y')
latest_date = dataframe['PurchaseDate'].max()
dataframe['Churn'] = (latest_date - dataframe['PurchaseDate']).dt.days > 365

# Balance the dataset by oversampling the minority class
from sklearn.utils import resample
churn_data = dataframe.groupby('Customer_ID').agg({
    'Age': 'mean',
    'Total_Value': 'sum',
    'Quantity': 'sum',
    'Churn': 'max'  # Target variable
}).dropna()

majority = churn_data[churn_data['Churn'] == False]
minority = churn_data[churn_data['Churn'] == True]
minority_upsampled = resample(minority, replace=True, n_samples=len(majority), random_state=42)
balanced_data = pd.concat([majority, minority_upsampled])

X_churn = balanced_data.drop('Churn', axis=1)
y_churn = balanced_data['Churn']

# Train-Test Split
X_train, X_test, y_train, y_test = train_test_split(X_churn, y_churn, test_size=0.2, random_state=42)

# Train a Random Forest Classifier
rf = RandomForestClassifier(random_state=42)
rf.fit(X_train, y_train)

# Predictions
y_pred = rf.predict(X_test)
print("Customer Churn Prediction Report:")
print(classification_report(y_test, y_pred))

# Preprocess data for Future Spending Trends
trend_data = dataframe.groupby(['Year', 'Month']).agg({
    'Total_Value': 'sum'
}).reset_index()

trend_data['Time'] = trend_data['Year'] * 12 + trend_data['Month']  # Convert Year/Month to a single timeline
X_trend = trend_data[['Time']]
y_trend = trend_data['Total_Value']

# Scale the data to normalize large values
scaler = MinMaxScaler()
X_trend_scaled = scaler.fit_transform(X_trend)

# Train-Test Split
X_train_trend, X_test_trend, y_train_trend, y_test_trend = train_test_split(X_trend_scaled, y_trend, test_size=0.2, random_state=42)

# Train a Gradient Boosting Regressor
gbr = GradientBoostingRegressor(random_state=42)
gbr.fit(X_train_trend, y_train_trend)

# Predictions
y_pred_trend = gbr.predict(X_test_trend)
print("\nFuture Spending Trends Prediction - MSE:")
print(mean_squared_error(y_test_trend, y_pred_trend))

# Preprocess data for Age or Location Spending Prediction
# Encode Location
label_encoder = LabelEncoder()
dataframe['Location_Encoded'] = label_encoder.fit_transform(dataframe['Location'])

age_location_data = dataframe[['Age', 'Location_Encoded', 'Total_Value']].dropna()

X_age_location = age_location_data[['Age', 'Location_Encoded']]
y_age_location = age_location_data['Total_Value']

# Train-Test Split
X_train_age, X_test_age, y_train_age, y_test_age = train_test_split(X_age_location, y_age_location, test_size=0.2, random_state=42)

# Train a Gradient Boosting Regressor for spending prediction
gbr_age = GradientBoostingRegressor(random_state=42)
gbr_age.fit(X_train_age, y_train_age)

# Predictions
y_pred_age = gbr_age.predict(X_test_age)
print("\nAge or Location Spending Prediction - MSE:")
print(mean_squared_error(y_test_age, y_pred_age))

def customer_segmentation(dataframe):
    # Group data by Customer_ID to calculate spend and purchase frequency
    customer_metrics = dataframe.groupby('Customer_ID').agg(
        Total_Spend=('Total_Value', 'sum'),
        Purchase_Frequency=('Customer_ID', 'count'),
        Age=('Age', 'first')
    ).reset_index()

    # Define high spenders: Customers with Total_Spend above the 75th percentile
    spend_threshold = customer_metrics['Total_Spend'].quantile(0.75)
    customer_metrics['Is_High_Spender'] = customer_metrics['Total_Spend'] > spend_threshold

    # Define frequent buyers: Customers with Purchase_Frequency above the 75th percentile
    frequency_threshold = customer_metrics['Purchase_Frequency'].quantile(0.75)
    customer_metrics['Is_Frequent_Buyer'] = customer_metrics['Purchase_Frequency'] > frequency_threshold

    # Add segmentation labels
    def segment_customer(row):
        if row['Is_High_Spender'] and row['Is_Frequent_Buyer']:
            return 'High Spender & Frequent Buyer'
        elif row['Is_High_Spender']:
            return 'High Spender'
        elif row['Is_Frequent_Buyer']:
            return 'Frequent Buyer'
        else:
            return 'Other'

    customer_metrics['Segment'] = customer_metrics.apply(segment_customer, axis=1)
    customer_metrics['Age_Group'] = dataframe['Age_Group']

    return customer_metrics
segmented_customers = customer_segmentation(dataframe)

def plot_customer_segmentation(segmented_customers):
    # Count the number of customers in each segment
    segment_counts = segmented_customers['Segment'].value_counts()

    # Plot a pie chart for customer segmentation
    plt.figure(figsize=(8, 8))
    plt.pie(segment_counts, labels=segment_counts.index, autopct='%1.1f%%', startangle=140)
    plt.title('Customer Segmentation Distribution')
    plt.tight_layout()
    plt.show()

    # Bar chart for spend and frequency by segment
    avg_metrics_by_segment = segmented_customers.groupby('Segment')[['Total_Spend', 'Purchase_Frequency']].mean()

    avg_metrics_by_segment.plot(kind='bar', figsize=(12, 6))
    plt.title('Average Spend and Frequency by Customer Segment')
    plt.ylabel('Average Value')
    plt.xlabel('Customer Segment')
    plt.tight_layout()
    plt.show()


# Plot the results
plot_customer_segmentation(segmented_customers)

def plot_pie_charts_by_segment(segmented_customers):
    # Group by segment and age group to calculate counts
    age_group_distribution = segmented_customers.groupby(['Segment', 'Age_Group']).size().unstack(fill_value=0)

    # Iterate through each segment to plot individual pie charts
    for segment in age_group_distribution.index:
        # Data for the current segment
        segment_data = age_group_distribution.loc[segment]

        # Plot pie chart
        plt.figure(figsize=(8, 8))
        segment_data.plot(kind='pie', autopct='%1.1f%%', startangle=140, legend=False)
        plt.title(f'Age Group Distribution for {segment}')
        plt.ylabel('')  # Remove y-axis label for better clarity
        plt.tight_layout()
        plt.show()

# Apply the function to plot the charts
plot_pie_charts_by_segment(segmented_customers)

# Redefine the function to find popular categories by age group
def find_popular_categories_by_age_group(dataframe, segmented_customers):
    # Merge original dataframe with segmentation results
    merged_data = pd.merge(dataframe, segmented_customers[['Customer_ID', 'Segment']], on='Customer_ID')

    # Group by Segment, Age_Group, and Category to count the number of sales
    popular_categories = merged_data.groupby(['Segment', 'Age_Group', 'Category']).size().reset_index(name='Sales')

    # Find the most popular category for each age group within each segment
    popular_categories = popular_categories.sort_values(['Segment', 'Age_Group', 'Sales'], ascending=[True, True, False])
    popular_categories = popular_categories.groupby(['Segment', 'Age_Group']).first().reset_index()

    return popular_categories

# Find popular categories in each segment and age group
popular_categories_by_age_group = find_popular_categories_by_age_group(dataframe, segmented_customers)

# Function to plot popular categories for each segment and age group
def plot_popular_categories(popular_categories):
    # Loop through each segment to plot graphs
    for segment in popular_categories['Segment'].unique():
        # Filter data for the current segment
        segment_data = popular_categories[popular_categories['Segment'] == segment]

        # Plot bar chart for the current segment
        plt.figure(figsize=(10, 6))
        for age_group in segment_data['Age_Group'].unique():
            # Filter data for the current age group
            age_data = segment_data[segment_data['Age_Group'] == age_group]
            plt.bar(age_data['Category'], age_data['Sales'], label=age_group)

        plt.title(f'Popular Categories in {segment}')
        plt.ylabel('Number of Sales')
        plt.xlabel('Category')
        plt.legend(title='Age Group')
        plt.tight_layout()
        plt.show()
# Plot the results
plot_popular_categories(popular_categories_by_age_group)

# Function to analyze city-age groups and recommend categories
def city_age_group_analysis_with_recommendations(dataframe, segmented_customers):
    merged_data = pd.merge(dataframe, segmented_customers[['Customer_ID']], on='Customer_ID')

    city_age_distribution = merged_data.groupby(['Location', 'Age_Group']).size().reset_index(name='Customer_Count')

    city_category_analysis = merged_data.groupby(['Location', 'Age_Group', 'Category']).size().reset_index(name='Sales')

    most_sold_categories = city_category_analysis.sort_values(
        ['Location', 'Age_Group', 'Sales'], ascending=[True, True, False]
    ).groupby(['Location', 'Age_Group']).first().reset_index()

    recommendations = pd.merge(city_age_distribution, most_sold_categories, on=['Location', 'Age_Group'])
    recommendations.rename(columns={'Category': 'Popular_Category', 'Sales': 'Top_Category_Sales'}, inplace=True)

    all_categories = dataframe['Category'].unique()
    recommendations['Recommended_Categories'] = recommendations.apply(
        lambda row: [cat for cat in all_categories if cat != row['Popular_Category']], axis=1
    )

    return recommendations

# # Perform the city-age group analysis and generate recommendations
recommendations = city_age_group_analysis_with_recommendations(dataframe, segmented_customers)

# Display recommendations
from IPython.display import display
display(recommendations)

def analyze_correlations(dataframe):
    # Add external factors: Is_Holiday and Weather_Score (example placeholder)
    dataframe['Is_Holiday'] = dataframe['isHoliday'].astype(int)  # Assuming 'isHoliday' is already present

    # Analyze correlations with Total_Value (sales)
    correlation_data = dataframe[['Total_Value', 'Is_Holiday']].corr()

    # Plot heatmap for correlations
    plt.figure(figsize=(8, 6))
    sns.heatmap(correlation_data, annot=True, cmap='coolwarm', fmt='.2f')
    plt.title('Correlation Between Sales and External Factors')
    plt.tight_layout()
    plt.show()

    return correlation_data

# Perform correlation analysis
correlation_results = analyze_correlations(dataframe)

sales_by_size = dataframe.groupby('Size')['Total_Value'].sum().reset_index()
sales_by_size.rename(columns={'Total_Value': 'Total_Sales'}, inplace=True)

# 2. Average Purchase Value by Store Size
avg_purchase_by_size = dataframe.groupby('Size')['Total_Value'].mean().reset_index()
avg_purchase_by_size.rename(columns={'Total_Value': 'Avg_Purchase_Value'}, inplace=True)

# 3. Customer Demographics by Store Size
customer_age_by_size = dataframe.groupby('Size')['Age'].mean().reset_index()
customer_gender_by_size = dataframe.groupby(['Size', 'Gender']).size().reset_index(name='Count')

# 4. Product Popularity by Store Size
popular_products_by_size = dataframe.groupby(['Size', 'Product_ID'])['Quantity'].sum().reset_index()
popular_products_by_size = popular_products_by_size.sort_values(['Size', 'Quantity'], ascending=[True, False])

# 5. Category Distribution by Store Size
category_by_size = dataframe.groupby(['Size', 'Category'])['Quantity'].sum().reset_index()
category_by_size = category_by_size.sort_values(['Size', 'Quantity'], ascending=[True, False])

sales_trends_by_size = dataframe.groupby(['Size', 'YearMonth'])['Total_Value'].sum().reset_index()

print("1. Sales Contribution by Store Size:")
print(sales_by_size)

print("\n2. Average Purchase Value by Store Size:")
print(avg_purchase_by_size)

print("\n3. Customer Age by Store Size:")
print(customer_age_by_size)

print("\n4. Customer Gender Distribution by Store Size:")
print(customer_gender_by_size)

print("\n5. Most Popular Products by Store Size:")
print(popular_products_by_size.head(10))  # Display top 10 products

print("\n6. Category Distribution by Store Size:")
print(category_by_size)

print("\n7. Sales Trends Over Time by Store Size:")
print(sales_trends_by_size.head(10))

"""```
# This is formatted as code
```

The regression coefficients represent the impact of each independent variable (Quantity, Price, and isHoliday) on the dependent variable (Total_Value or sales):

Quantity: 108.06

For every additional unit sold, the sales increase by approximately 108.06 units of currency (e.g., dollars).
Indicates a strong positive relationship between quantity sold and sales.
Price: 19.30

For every unit increase in price, the sales increase by 19.30 units of currency.
This suggests that higher prices slightly increase total sales, possibly due to higher-value products or premium pricing.
isHoliday: -11.88

When a transaction occurs on a holiday, sales decrease by approximately 11.88 units of currency.
Suggests that holidays might negatively impact sales, likely due to store closures, reduced demand, or other factors.

R-Squared Value
R-squared: 0.378
The R-squared value indicates that approximately 37.8% of the variance in sales (Total_Value) is explained by the independent variables (Quantity, Price, and isHoliday).
While the model captures a moderate amount of variance, there is still 62.2% variance unexplained, suggesting other factors (e.g., promotions, location, customer demographics) might influence sales.
"""

import seaborn as sns
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score
regression_data = dataframe[['Total_Value', 'Quantity', 'Price', 'isHoliday']]
regression_data['isHoliday'] = regression_data['isHoliday'].astype(int)  # Ensure binary data for holidays

# Define features (X) and target (y)
X = regression_data[['Quantity', 'Price', 'isHoliday']]
y = regression_data['Total_Value']

# Fit a linear regression model
reg_model = LinearRegression()
reg_model.fit(X, y)

# Predict and calculate R-squared
y_pred = reg_model.predict(X)
r2 = r2_score(y, y_pred)

print("Regression Coefficients:")
print(dict(zip(X.columns, reg_model.coef_)))
print(f"R-squared: {r2}")

# Print Regression Coefficients and R-squared
print("Regression Coefficients:")
print(dict(zip(X.columns, reg_model.coef_)))
print(f"R-squared: {r2}")

# Bar Chart for Coefficients
plt.figure(figsize=(10, 6))
bars = plt.bar(X.columns, reg_model.coef_, color=['skyblue', 'lightgreen', 'salmon'])

# Add data labels
for bar in bars:
    yval = bar.get_height()
    plt.text(bar.get_x() + bar.get_width() / 2, yval, f'{yval:.2f}', ha='center', va='bottom', fontsize=12)

plt.title('Regression Coefficients', fontsize=16)
plt.xlabel('Features', fontsize=14)
plt.ylabel('Coefficient Value', fontsize=14)
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.tight_layout()
plt.show()

# Visualize regression coefficients
coefficients = dict(zip(X.columns, reg_model.coef_))
plt.figure(figsize=(8, 6))
plt.bar(coefficients.keys(), coefficients.values(), color='skyblue')
plt.title('Regression Coefficients')
plt.xlabel('Features')
plt.ylabel('Coefficient Value')
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.tight_layout()
plt.show()

# Visualize actual vs predicted Total_Value
plt.figure(figsize=(10, 6))
plt.scatter(y, y_pred, alpha=0.6, color='blue', label='Predicted vs Actual')
plt.plot([y.min(), y.max()], [y.min(), y.max()], '--', color='red', label='Perfect Fit')
plt.title('Actual vs Predicted Sales (Total_Value)')
plt.xlabel('Actual Sales (Total_Value)')
plt.ylabel('Predicted Sales (Total_Value)')
plt.legend()
plt.grid(alpha=0.5)
plt.tight_layout()
plt.show()

"""This scatter plot represents customer segmentation using K-Means Clustering. Here's what it means:

Axes:

The X-axis shows the Total Spend by customers (how much money they've spent).
The Y-axis represents the Purchase Frequency (how often they buy).
Clusters:

The data points are divided into 4 clusters, indicated by different colors (e.g., 0, 1, 2, 3).
Each cluster groups customers with similar spending behavior and purchase frequency.
Insights:

Cluster 0 (Blue): Likely represents low spenders with infrequent purchases.
Cluster 1 (Purple): Represents slightly higher frequency but still lower spend.
Cluster 2 (Green): Represents customers with moderate to high spend and purchase frequency.
Cluster 3 (Yellow): Likely high-frequency buyers with moderate spend.
Purpose:
This clustering helps businesses understand customer behavior and:

Identify high-value customers (e.g., Cluster 2 or 3) for loyalty programs.
Design strategies to engage low-frequency buyers (e.g., Cluster 0 or 1) with targeted promotions or offers.
"""

from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
# Create customer-level data
customer_metrics = dataframe.groupby('Customer_ID').agg(
        Total_Spend=('Total_Value', 'sum'),
        Purchase_Frequency=('Customer_ID', 'count')
        ).reset_index()

# Standardize the data
scaler = StandardScaler()
metrics_scaled = scaler.fit_transform(customer_metrics[['Total_Spend', 'Purchase_Frequency']])

# Apply K-Means clustering
kmeans = KMeans(n_clusters=4, random_state=42)
customer_metrics['Cluster'] = kmeans.fit_predict(metrics_scaled)

print("Cluster Centers:")
print(kmeans.cluster_centers_)

# Visualize clusters
plt.figure(figsize=(10, 6))
sns.scatterplot(
        x=customer_metrics['Total_Spend'],
        y=customer_metrics['Purchase_Frequency'],
        hue=customer_metrics['Cluster'],
        palette='viridis'
)
plt.title('Customer Segmentation (K-Means Clustering)')
plt.xlabel('Total Spend')
plt.ylabel('Purchase Frequency')
plt.show()

product_performance = dataframe.groupby('Product_ID')['Total_Value'].sum().reset_index()
low_performing_products = product_performance.sort_values(by='Total_Value').head(10)

# Identify low-performing stores
store_performance = dataframe.groupby('Store_ID')['Total_Value'].sum().reset_index()
low_performing_stores = store_performance.sort_values(by='Total_Value').head(10)

# Recommendations
recommendations = []
for _, row in low_performing_products.iterrows():
        recommendations.append(f"Consider reviewing pricing or promotions for Product {row['Product_ID']}.")
for _, row in low_performing_stores.iterrows():
        recommendations.append(f"Evaluate marketing strategies or inventory management for Store {row['Store_ID']}.")

print("Recommendations for Low Performers:")
print("\n".join(recommendations))

from statsmodels.tsa.holtwinters import ExponentialSmoothing
from sklearn.metrics import mean_squared_error
import numpy as np

# Prepare data for time-series forecasting
sales_data = dataframe.groupby('YearMonth')['Total_Value'].sum().reset_index()
sales_data['YearMonth']
sales_data.set_index('YearMonth', inplace=True)
sales_data.index = sales_data.index.to_timestamp()

# Train-test split
train_data = sales_data.iloc[:-12]
test_data = sales_data.iloc[-12:]

# Fit Exponential Smoothing model
model = ExponentialSmoothing(train_data, seasonal='add', seasonal_periods=12).fit()

# Forecast
forecast = model.forecast(steps=12)

# Evaluate performance
mse = mean_squared_error(test_data, forecast)

# Root Mean Squared Error (RMSE)
rmse = np.sqrt(mse)

# Mean Absolute Percentage Error (MAPE)
mape = np.mean(np.abs((test_data.values - forecast.values) / test_data.values)) * 100
accuracy_score = 100 - mape

# Print the evaluation metrics
print(f"Mean Squared Error (MSE): {mse:.2f}")
print(f"Root Mean Squared Error (RMSE): {rmse:.2f}")
print(f"Mean Absolute Percentage Error (MAPE): {mape:.2f}%")
print(f"Accuracy Score: {accuracy_score:.2f}%")


print(f"Mean Squared Error: {mse}")

plt.figure(figsize=(12, 6))

# Plot actual sales
plt.plot(sales_data, label='Actual Sales', color='blue', marker='o')

# Plot forecasted sales
plt.plot(forecast.index, forecast, label='Forecast', color='orange', linestyle='--', marker='x')

# Add a vertical line to separate actual vs forecast
plt.axvline(sales_data.index[-1], color='red', linestyle='--', label='Forecast Start')

# Add labels, legend, and title
plt.title('Actual vs Forecasted Sales')
plt.xlabel('Date')
plt.ylabel('Total Sales Value')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()


plt.figure(figsize=(12, 6))

# Forecast sales for the next 6 months
forecast_6_months = model.forecast(steps=6)

# Create a date range for the forecasted months
forecast_index = pd.date_range(start=test_data.index[-1] + pd.DateOffset(months=1), periods=6, freq='M')

# Display the forecasted values
print("6-Month Sales Forecast:")
print(pd.DataFrame({'Date': forecast_index, 'Forecasted Sales': forecast_6_months.values}))

# Visualization
plt.figure(figsize=(12, 6))

# Plot actual sales
plt.plot(sales_data, label='Actual Sales', color='blue', marker='o')

# Plot forecasted sales for the next 6 months
plt.plot(forecast_index, forecast_6_months, label='6-Month Forecast', color='green', linestyle='--', marker='x')

# Add a vertical line to separate actual vs forecast
plt.axvline(sales_data.index[-1], color='red', linestyle='--', label='Forecast Start')

# Add labels, legend, and title
plt.title('6-Month Sales Forecast')
plt.xlabel('Date')
plt.ylabel('Total Sales Value')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

output_file_path = "final_analysis_results.csv"

# Save the DataFrame to a CSV file
dataframe.to_csv(output_file_path, index=False)